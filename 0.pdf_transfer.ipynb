{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3cccfbf4",
      "metadata": {},
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import os\n",
        "from tqdm import tqdm \n",
        "import openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7838198d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_filename_without_extension(file_path):\n",
        "    return os.path.splitext(os.path.basename(file_path))[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "785d1c49",
      "metadata": {},
      "outputs": [],
      "source": [
        "#フォルダパス内のファイルパスを取得する関数\n",
        "def get_file_paths(folder_path):\n",
        "    file_paths = []\n",
        "    for root, _, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            file_path = os.path.join(root, file)\n",
        "            file_paths.append(file_path)\n",
        "    return file_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "aa498401",
      "metadata": {},
      "outputs": [],
      "source": [
        "#PDFからドキュメントを取得\n",
        "def read_pdf(file_path):\n",
        "    pdf_document = fitz.open(file_path)\n",
        "    full_text = \"\"\n",
        "    for page_num in range(pdf_document.page_count):\n",
        "        page = pdf_document.load_page(page_num)\n",
        "        full_text += page.get_text(\"text\")\n",
        "    return full_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "e8935c60",
      "metadata": {},
      "outputs": [],
      "source": [
        "#set_path\n",
        "\n",
        "pdf_folder_path = \"pdf/\"\n",
        "text_folder_path = \"text/\"\n",
        "pdf_text_folder_path = text_folder_path + \"pdf_text/\"\n",
        "pdf_summarize_text_folder_path = text_folder_path + \"pdf_summarize_text/\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f3ee429",
      "metadata": {},
      "source": [
        "### 1.abstract text from pdf\n",
        "- https://qiita.com/akiraokusawa/items/ba83893669484d33067c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "1486d0da",
      "metadata": {},
      "outputs": [],
      "source": [
        "# get_path\n",
        "pdf_paths = get_file_paths(pdf_folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "05506953",
      "metadata": {},
      "outputs": [],
      "source": [
        "# #save_text\n",
        "# for pdf_path in tqdm(pdf_paths):\n",
        "#     text = read_pdf(pdf_path)\n",
        "#     file_name = get_filename_without_extension(pdf_path)\n",
        "#     with open( pdf_text_folder_path+ f'{file_name}.txt', 'w', encoding='utf-8') as f:\n",
        "#         f.write(text)\n",
        "#         f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1cb9bbb",
      "metadata": {},
      "source": [
        "### 2.summarize text with 4omini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "4d0937a6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import base64\n",
        "import numpy as np\n",
        "from openai import AzureOpenAI\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import time\n",
        "\n",
        "import re\n",
        "from janome.tokenizer import Tokenizer\n",
        "import unicodedata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "8cc951ec",
      "metadata": {},
      "outputs": [],
      "source": [
        "#set env\n",
        "AZURE_OPENAI_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
        "AZURE_OPENAI_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "API_VERSION = os.getenv(\"API_VERSION\")\n",
        "DEPLOYMENT_ID_FOR_CHAT_COMPLETION = os.getenv(\"DEPLOYMENT_ID_FOR_CHAT_COMPLETION\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "7029ae1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_text_from_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "c0107e1d",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def generate_summary(text):\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Summarize the following text : \\\"{text}\\\"\"}\n",
        "        # {\"role\": \"user\", \"content\": f\"Summarize the following text with emphasis on keywords like revenue, profit, sales performance, market trends, issues, improvements, and personnel changes: \\\"{text}\\\"\"}\n",
        "    ]\n",
        "\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message[\"content\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "2d4e39ac",
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize_text(text):\n",
        "    # content =  [\n",
        "    #     {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "\n",
        "    #     {\"role\": \"user\", \"content\": f\"Summarize the following text : \\\"{text}\\\"\"}\n",
        "    #     ]\n",
        "\n",
        "    content =  [\n",
        "        {\"role\": \"system\", \"content\": \"あなたはテキストを要約してmarkdown形式で出力します。\"},\n",
        "\n",
        "        {\"role\": \"user\", \"content\": f\"次の文章を要約してください。ただし特殊な記号や改行コードは削除してから要約してください。 : \\\"{text}\\\"\"}\n",
        "        ]\n",
        "\n",
        "\n",
        "    client = AzureOpenAI(\n",
        "        api_key=AZURE_OPENAI_API_KEY,\n",
        "        api_version=API_VERSION,\n",
        "        azure_endpoint=AZURE_OPENAI_ENDPOINT)\n",
        "\n",
        "    completion_resp = client.chat.completions.create(\n",
        "        model=DEPLOYMENT_ID_FOR_CHAT_COMPLETION,\n",
        "        temperature=0,\n",
        "        seed=42,\n",
        "        messages=content\n",
        "        )\n",
        "    \n",
        "    return completion_resp.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "2e18f2f6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# テキストの正規化\n",
        "def normalize_text(text):\n",
        "    # 全角文字を半角文字に変換\n",
        "    text = unicodedata.normalize('NFKC', text)\n",
        "    return text\n",
        "\n",
        "# 特殊文字と改行コードの除去\n",
        "def remove_special_characters(text):\n",
        "    # 改行コード、特殊文字、数字を除去（必要に応じて正規表現を調整）\n",
        "    text = re.sub(r'\\n|\\r', '', text)\n",
        "    text = re.sub(r'[^ぁ-んァ-ン一-龥a-zA-Z0-9\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "def contains_hiragana(text):\n",
        "    # ひらがなを判定するための正規表現\n",
        "    hiragana_pattern = re.compile('[\\u3040-\\u309F]')\n",
        "    \n",
        "    # テキストにひらがなが含まれるかをチェック\n",
        "    if hiragana_pattern.search(text):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def delete_few_info_lines(text):\n",
        "    # 改行コードで分割してリストに格納\n",
        "    lines = text.split('\\n')\n",
        "    # 10文字以下の行を削除\n",
        "    filtered_lines = [line for line in lines if len(line) > 20]\n",
        "    # フィルタリングされた行を再結合して1つの文字列にする\n",
        "    result = '\\n'.join(filtered_lines)\n",
        "    return result\n",
        "\n",
        "def delete_url_lines(text):\n",
        "    # 改行コードで分割してリストに格納\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    filtered_lines = [line for line in lines if not \"http\" in line]\n",
        "\n",
        "    # フィルタリングされた行を再結合して1つの文字列にする\n",
        "    result = '\\n'.join(filtered_lines)\n",
        "    return result\n",
        "\n",
        "def delete_reference_lines(text):\n",
        "    # 改行コードで分割してリストに格納\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    filtered_lines = [line for line in lines if not line.startswith(\"※\")]\n",
        "\n",
        "    # フィルタリングされた行を再結合して1つの文字列にする\n",
        "    result = '\\n'.join(filtered_lines)\n",
        "    return result\n",
        "\n",
        "def delete_not_document_lines(text):\n",
        "    #ひらがなが含まれない（文章ではない）行を削除\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    filtered_lines = [line for line in lines if contains_hiragana(line)]\n",
        "\n",
        "    # フィルタリングされた行を再結合して1つの文字列にする\n",
        "    result = '\\n'.join(filtered_lines)\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "# 前処理関数\n",
        "def preprocess_japanese_text(text):\n",
        "    text = normalize_text(text)\n",
        "    text = delete_few_info_lines(text)\n",
        "    text = delete_not_document_lines(text)\n",
        "    text = delete_url_lines(text)\n",
        "    text = delete_reference_lines(text)\n",
        "    text = remove_special_characters(text)\n",
        "    \n",
        "    # tokenizer = Tokenizer()\n",
        "    # tokens = tokenizer.tokenize(text, wakati=True)\n",
        "    # processed_text = ' '.join(tokens)\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "e638f51a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "\n",
        "encoding_35 = tiktoken.encoding_for_model(\"gpt-3.5-turbo-0301\")\n",
        "encoding_4 = tiktoken.encoding_for_model(\"gpt-4-0314\")\n",
        "\n",
        "def calc_token(chat, encoding_name):\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(chat))\n",
        "    return num_tokens\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "5c87d5ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# get_path\n",
        "text_paths = get_file_paths(pdf_text_folder_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "e72688da",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/19 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 10411 11790\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 1/19 [00:05<01:36,  5.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10 166163 185877\n",
            "10 165163 184816\n",
            "10 164163 183701\n",
            "10 163163 182584\n",
            "10 162163 181519\n",
            "10 161163 180469\n",
            "10 160163 179371\n",
            "10 159163 178474\n",
            "10 158163 177423\n",
            "10 157163 176276\n",
            "10 156163 175143\n",
            "10 155163 174019\n",
            "10 154163 172850\n",
            "10 153163 171614\n",
            "10 152163 170454\n",
            "10 151163 169304\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|█         | 2/19 [00:41<06:35, 23.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11 52504 59891\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 3/19 [00:51<04:38, 17.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12 40326 45840\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 21%|██        | 4/19 [01:00<03:31, 14.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13 38793 44129\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|██▋       | 5/19 [01:07<02:39, 11.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14 26710 30953\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|███▏      | 6/19 [01:14<02:11, 10.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15 35233 41948\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 37%|███▋      | 7/19 [01:22<01:50,  9.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16 52048 58367\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 8/19 [01:31<01:41,  9.23s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17 55249 61399\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 47%|████▋     | 9/19 [01:42<01:37,  9.79s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "18 46975 51860\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 53%|█████▎    | 10/19 [01:55<01:36, 10.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "19 63901 71378\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 58%|█████▊    | 11/19 [02:06<01:28, 11.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2 70738 79481\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 63%|██████▎   | 12/19 [02:20<01:22, 11.81s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3 126192 140116\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 68%|██████▊   | 13/19 [02:32<01:11, 11.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4 60372 67921\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 74%|███████▎  | 14/19 [02:40<00:53, 10.72s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5 37690 42958\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 79%|███████▉  | 15/19 [02:48<00:38,  9.71s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6 52329 60180\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 84%|████████▍ | 16/19 [02:58<00:29,  9.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7 67461 76491\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 89%|████████▉ | 17/19 [03:08<00:19,  9.93s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8 148998 163103\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 95%|█████████▍| 18/19 [03:35<00:15, 15.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9 55850 63394\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 19/19 [03:46<00:00, 11.93s/it]\n"
          ]
        }
      ],
      "source": [
        "#save_text\n",
        "for text_path in tqdm(text_paths):\n",
        "    file_name = get_filename_without_extension(text_path)\n",
        "    text = get_text_from_file(text_path)\n",
        "    text = text.replace(\" \",\"\")\n",
        "    text = preprocess_japanese_text(text)\n",
        "\n",
        "\n",
        "    while True:\n",
        "        token = calc_token(text, encoding_4.name)\n",
        "        print(file_name,len(text),token)\n",
        "        if token <170000:\n",
        "            break\n",
        "        else:\n",
        "            text = text[:-1000]\n",
        "\n",
        "    calc_token(text, encoding_4.name)\n",
        "\n",
        "    summary = summarize_text(text)\n",
        "    with open( pdf_summarize_text_folder_path+ f'{file_name}.txt', 'w', encoding='utf-8') as f:\n",
        "        f.write(summary)\n",
        "        f.close()\n",
        "    "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
